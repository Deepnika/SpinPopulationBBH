{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "commercial-foundation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import *\n",
    "from plots import *\n",
    "\n",
    "xp = gwpop.cupy_utils.xp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-bishop",
   "metadata": {},
   "source": [
    "### Posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "arranged-highway",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = pd.read_pickle(\"bbh_spin/posterior_samples/posteriors_1.pkl\") + pd.read_pickle(\"bbh_spin/posterior_samples/posteriors_2.pkl\")\n",
    "posts_zero = pd.read_pickle(\"bbh_spin/posterior_samples/posteriors_zero_1.pkl\") + pd.read_pickle(\"bbh_spin/posterior_samples/posteriors_zero_2.pkl\")\n",
    "\n",
    "event_ids = []\n",
    "ff = open(\"bbh_spin/posterior_samples/event_list.txt\",\"r\")\n",
    "for line in ff.read().splitlines():\n",
    "    event_ids.append(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "killing-cooking",
   "metadata": {},
   "source": [
    "### Extended Model without selection effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "damaged-navigator",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gwpopulation.utils import beta_dist, truncnorm\n",
    "\n",
    "def iid_spin_magnitude_beta_truncated_gaussian(dataset, amax=1, alpha_chi=1, beta_chi=1, lambda_chi_peak=0, sigma_chi_peak=0):\n",
    "\n",
    "    return independent_spin_magnitude_beta_truncated_gaussian(\n",
    "        dataset, alpha_chi, alpha_chi, beta_chi, beta_chi, amax, amax, lambda_chi_peak, lambda_chi_peak, sigma_chi_peak, sigma_chi_peak)\n",
    "\n",
    "def independent_spin_magnitude_beta_truncated_gaussian(\n",
    "    dataset, alpha_chi_1, alpha_chi_2, beta_chi_1, beta_chi_2, amax_1, amax_2, lambda_chi_peak_1, lambda_chi_peak_2, sigma_chi_peak_1, sigma_chi_peak_2):\n",
    " \n",
    "    if alpha_chi_1 < 0 or beta_chi_1 < 0 or alpha_chi_2 < 0 or beta_chi_2 < 0:\n",
    "        return 0\n",
    "    \n",
    "    if (sigma_chi_peak_1 == 0 or sigma_chi_peak_2 == 0):\n",
    "        prior = (\n",
    "            (1 - lambda_chi_peak_1) * beta_dist(dataset[\"a_1\"], alpha_chi_1, beta_chi_1, scale=amax_1)\n",
    "                 + lambda_chi_peak_1 * np.nan_to_num(np.multiply((dataset[\"a_1\"] == 0), np.inf))\n",
    "        ) * (\n",
    "            (1 - lambda_chi_peak_2) * beta_dist(dataset[\"a_2\"], alpha_chi_2, beta_chi_2, scale=amax_2)\n",
    "                 + lambda_chi_peak_2 * np.nan_to_num(np.multiply((dataset[\"a_2\"] == 0), np.inf))\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        prior = (\n",
    "            (1 - lambda_chi_peak_1) * beta_dist(dataset[\"a_1\"], alpha_chi_1, beta_chi_1, scale=amax_1) \n",
    "            + lambda_chi_peak_1 * truncnorm(dataset[\"a_1\"], mu=0, sigma=sigma_chi_peak_1, low=0, high=amax_1)\n",
    "        ) * (\n",
    "            (1 - lambda_chi_peak_2) * beta_dist(dataset[\"a_2\"], alpha_chi_2, beta_chi_2, scale=amax_2) \n",
    "            + lambda_chi_peak_2 * truncnorm(dataset[\"a_2\"], mu=0, sigma=sigma_chi_peak_2, low=0, high=amax_2)\n",
    "        )\n",
    "\n",
    "    return prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "classified-nightlife",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Likelihoods for population inference\n",
    "\"\"\"\n",
    "\n",
    "import types\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import gamma\n",
    "from tqdm import tqdm\n",
    "\n",
    "from bilby.core.utils import logger\n",
    "from bilby.core.likelihood import Likelihood\n",
    "from bilby.hyper.model import Model\n",
    "\n",
    "from gwpopulation.models import mass, redshift\n",
    "\n",
    "from gwpopulation.cupy_utils import CUPY_LOADED, to_numpy, xp\n",
    "\n",
    "INF = xp.nan_to_num(xp.inf)\n",
    "\n",
    "\n",
    "class HyperparameterLikelihood(Likelihood):\n",
    "    \"\"\"\n",
    "    A likelihood for inferring hyperparameter posterior distributions with\n",
    "    including selection effects.\n",
    "    See Eq. (34) of https://arxiv.org/abs/1809.02293 for a definition.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        posteriors1,\n",
    "        posteriors2,\n",
    "        hyper_prior1,\n",
    "        hyper_prior2,\n",
    "        ln_evidences=None,\n",
    "        max_samples=1e100,\n",
    "        selection_function=lambda args: 1,\n",
    "        conversion_function=lambda args: (args, None),\n",
    "        cupy=True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        posteriors: list\n",
    "            An list of pandas data frames of samples sets of samples.\n",
    "            Each set may have a different size.\n",
    "            These can contain a `prior` column containing the original prior\n",
    "            values.\n",
    "        hyper_prior: `bilby.hyper.model.Model`\n",
    "            The population model, this can alternatively be a function.\n",
    "        ln_evidences: list, optional\n",
    "            Log evidences for single runs to ensure proper normalisation\n",
    "            of the hyperparameter likelihood. If not provided, the original\n",
    "            evidences will be set to 0. This produces a Bayes factor between\n",
    "            the sampling power_prior and the hyperparameterised model.\n",
    "        selection_function: func\n",
    "            Function which evaluates your population selection function.\n",
    "        conversion_function: func\n",
    "            Function which converts a dictionary of sampled parameter to a\n",
    "            dictionary of parameters of the population model.\n",
    "        max_samples: int, optional\n",
    "            Maximum number of samples to use from each set.\n",
    "        cupy: bool\n",
    "            If True and a compatible CUDA environment is available,\n",
    "            cupy will be used for performance.\n",
    "            Note: this requires setting up your hyper_prior properly.\n",
    "        \"\"\"\n",
    "        if cupy and not CUPY_LOADED:\n",
    "            logger.warning(\"Cannot import cupy, falling back to numpy.\")\n",
    "\n",
    "        #self.samples_per_posterior1 = max_samples\n",
    "        #self.samples_per_posterior2 = max_samples\n",
    "        self.data1, self.samples_per_posterior1 = self.resample_posteriors(posteriors1, max_samples=max_samples)\n",
    "        self.data2, self.samples_per_posterior2 = self.resample_posteriors(posteriors2, max_samples=max_samples)\n",
    "        logger.info(f\"samples_per_posterior1 {self.samples_per_posterior1}, samples_per_posterior2 {self.samples_per_posterior2}\")\n",
    " \n",
    "        '''\n",
    "        if isinstance(hyper_prior, types.FunctionType):\n",
    "            hyper_prior = Model([hyper_prior])\n",
    "        elif not (\n",
    "            hasattr(hyper_prior, \"parameters\")\n",
    "            and callable(getattr(hyper_prior, \"prob\"))\n",
    "        ):\n",
    "            raise AttributeError(\n",
    "                \"hyper_prior must either be a function, \"\n",
    "                \"or a class with attribute 'parameters' and method 'prob'\"\n",
    "            )\n",
    "        '''\n",
    "\n",
    "        self.hyper_prior1 = hyper_prior1 #Model([hyper_prior1])\n",
    "        self.hyper_prior2 = hyper_prior2 #Model([hyper_prior2]) #Model([mass.power_law_primary_mass_ratio, redshift.PowerLawRedshift])\n",
    "        \n",
    "        Likelihood.__init__(self, hyper_prior1.parameters)\n",
    "\n",
    "        if \"prior\" in self.data1:\n",
    "            self.sampling_prior1 = self.data1.pop(\"prior\")\n",
    "            self.sampling_prior2 = self.data2.pop(\"prior\")\n",
    "        else:\n",
    "            logger.info(\"No prior values provided, defaulting to 1.\")\n",
    "            self.sampling_prior = 1\n",
    "\n",
    "        if ln_evidences is not None:\n",
    "            self.total_noise_evidence = np.sum(ln_evidences)\n",
    "        else:\n",
    "            self.total_noise_evidence = np.nan\n",
    "\n",
    "        ln_evidences1 = self.data1.pop(\"ln_evidence\")\n",
    "        ln_evidences2 = self.data2.pop(\"ln_evidence\")\n",
    "        self.total_evidence1 = xp.asarray([val[0] for val in ln_evidences1])\n",
    "        self.total_evidence2 = xp.asarray([val[0] for val in ln_evidences2])\n",
    "        \n",
    "        logger.info(f\"log evidences = {self.total_evidence1}\")\n",
    "        logger.info(f\"log evidences = {self.total_evidence2}\")\n",
    "\n",
    "        self.data2.pop(\"a_1\")\n",
    "        self.data2.pop(\"a_2\")\n",
    "        self.data2.pop(\"cos_tilt_1\")\n",
    "        self.data2.pop(\"cos_tilt_2\")\n",
    "        \n",
    "        self.conversion_function = conversion_function\n",
    "        self.selection_function = selection_function\n",
    "\n",
    "        self.n_posteriors = len(posteriors1)\n",
    "\n",
    "    __doc__ += __init__.__doc__\n",
    "\n",
    "    def log_likelihood_ratio(self):\n",
    "        self.parameters, added_keys = self.conversion_function(self.parameters)\n",
    "        self.hyper_prior1.parameters.update(self.parameters)\n",
    "        self.hyper_prior2.parameters.update(self.parameters)\n",
    "        ln_l = xp.sum(self._compute_per_event_ln_bayes_factors())\n",
    "        ln_l += self._get_selection_factor()\n",
    "        if added_keys is not None:\n",
    "            for key in added_keys:\n",
    "                self.parameters.pop(key)\n",
    "        if xp.isnan(ln_l):\n",
    "            return float(-INF)\n",
    "        else:\n",
    "            return float(xp.nan_to_num(ln_l))\n",
    "\n",
    "    def noise_log_likelihood(self):\n",
    "        return self.total_noise_evidence\n",
    "\n",
    "    def log_likelihood(self):\n",
    "        return self.noise_log_likelihood() + self.log_likelihood_ratio()\n",
    "\n",
    "    def _compute_per_event_ln_bayes_factors(self):\n",
    "        term1 = xp.sum(self.hyper_prior1.prob(self.data1) / self.sampling_prior1, axis=-1)/self.samples_per_posterior1\n",
    "        term2 = xp.sum(self.hyper_prior2.prob(self.data2) / self.sampling_prior2, axis=-1)/self.samples_per_posterior2 * xp.exp(self.total_evidence2 - self.total_evidence1)\n",
    "        ln_l = xp.log((1. - self.parameters[\"lambda_chi_peak\"])*term1 + self.parameters[\"lambda_chi_peak\"]*term2)\n",
    "        return ln_l\n",
    "\n",
    "    def _get_selection_factor(self):\n",
    "        return -self.n_posteriors * xp.log(self.selection_function(self.parameters))\n",
    "\n",
    "    def generate_extra_statistics(self, sample):\n",
    "        \"\"\"\n",
    "        Given an input sample, add extra statistics\n",
    "        Adds the ln BF for each of the events in the data and the selection\n",
    "        function\n",
    "        Parameters\n",
    "        ----------\n",
    "        sample: dict\n",
    "            Input sample to compute the extra things for.\n",
    "        Returns\n",
    "        -------\n",
    "        sample: dict\n",
    "            The input dict, modified in place.\n",
    "        \"\"\"\n",
    "        self.parameters.update(sample.copy())\n",
    "        self.parameters, added_keys = self.conversion_function(self.parameters)\n",
    "        self.hyper_prior1.parameters.update(self.parameters)\n",
    "        self.hyper_prior2.parameters.update(self.parameters)\n",
    "        logger.info(self.parameters)\n",
    "        #self.parameters.pop('')\n",
    "        ln_ls = self._compute_per_event_ln_bayes_factors()\n",
    "        for ii in range(self.n_posteriors):\n",
    "            sample[f\"ln_bf_{ii}\"] = float(ln_ls[ii])\n",
    "        sample[\"selection\"] = float(self.selection_function(self.parameters))\n",
    "        if added_keys is not None:\n",
    "            for key in added_keys:\n",
    "                self.parameters.pop(key)\n",
    "        return sample\n",
    "\n",
    "    def generate_rate_posterior_sample(self):\n",
    "        r\"\"\"\n",
    "        Generate a sample from the posterior distribution for rate assuming a\n",
    "        :math:`1 / R` prior.\n",
    "        The likelihood evaluated is analytically marginalized over rate.\n",
    "        However the rate dependent likelihood can be trivially written.\n",
    "        .. math::\n",
    "            p(R) = \\Gamma(n=N, \\text{scale}=\\mathcal{V})\n",
    "        Here :math:`\\Gamma` is the Gamma distribution, :math:`N` is the number\n",
    "        of events being analyzed and :math:`\\mathcal{V}` is the total observed 4-volume.\n",
    "        Returns\n",
    "        -------\n",
    "        rate: float\n",
    "            A sample from the posterior distribution for rate.\n",
    "        \"\"\"\n",
    "        if hasattr(self.selection_function, \"detection_efficiency\") and hasattr(\n",
    "            self.selection_function, \"surveyed_hypervolume\"\n",
    "        ):\n",
    "            efficiency, _ = self.selection_function.detection_efficiency(\n",
    "                self.parameters\n",
    "            )\n",
    "            vt = efficiency * self.selection_function.surveyed_hypervolume(\n",
    "                self.parameters\n",
    "            )\n",
    "        else:\n",
    "            vt = self.selection_function(self.parameters)\n",
    "        rate = gamma(a=self.n_posteriors).rvs() / vt\n",
    "        return rate\n",
    "\n",
    "    def resample_posteriors(self, posteriors, max_samples=1e300):\n",
    "        \"\"\"\n",
    "        Convert list of pandas DataFrame object to dict of arrays.\n",
    "        Parameters\n",
    "        ----------\n",
    "        posteriors: list\n",
    "            List of pandas DataFrame objects.\n",
    "        max_samples: int, opt\n",
    "            Maximum number of samples to take from each posterior,\n",
    "            default is length of shortest posterior chain.\n",
    "        Returns\n",
    "        -------\n",
    "        data: dict\n",
    "            Dictionary containing arrays of size (n_posteriors, max_samples)\n",
    "            There is a key for each shared key in posteriors.\n",
    "        \"\"\"\n",
    "        for posterior in posteriors:\n",
    "            max_samples = min(len(posterior), max_samples)\n",
    "        data = {key: [] for key in posteriors[0]}\n",
    "        logger.debug(f\"Downsampling to {max_samples} samples per posterior.\")\n",
    "        samples_per_posterior = max_samples\n",
    "        for posterior in posteriors:\n",
    "            temp = posterior.sample(samples_per_posterior)\n",
    "            for key in data:\n",
    "                data[key].append(temp[key])\n",
    "        for key in data:\n",
    "            data[key] = xp.array(data[key])\n",
    "        return data, samples_per_posterior\n",
    "\n",
    "    def posterior_predictive_resample(self, samples, return_weights=False):\n",
    "        \"\"\"\n",
    "        Resample the original single event posteriors to use the PPD from each\n",
    "        of the other events as the prior.\n",
    "        There may be something weird going on with rate.\n",
    "        Parameters\n",
    "        ----------\n",
    "        samples: pd.DataFrame, dict, list\n",
    "            The samples to do the weighting over, typically the posterior from\n",
    "            some run.\n",
    "        return_weights: bool, optional\n",
    "            Whether to return the per-sample weights, default = False\n",
    "        Returns\n",
    "        -------\n",
    "        new_samples: dict\n",
    "            Dictionary containing the weighted posterior samples for each of\n",
    "            the events.\n",
    "        weights: array-like\n",
    "            Weights to apply to the samples, only if return_weights == True.\n",
    "        \"\"\"\n",
    "        if isinstance(samples, pd.DataFrame):\n",
    "            samples = [dict(samples.iloc[ii]) for ii in range(len(samples))]\n",
    "        elif isinstance(samples, dict):\n",
    "            samples = [samples]\n",
    "        weights = xp.zeros((self.n_posteriors, self.samples_per_posterior))\n",
    "        event_weights = xp.zeros(self.n_posteriors)\n",
    "        for sample in tqdm(samples):\n",
    "            self.parameters.update(sample.copy())\n",
    "            self.parameters, added_keys = self.conversion_function(self.parameters)\n",
    "            new_weights = self.hyper_prior.prob(self.data) / self.sampling_prior\n",
    "            event_weights += xp.mean(new_weights, axis=-1)\n",
    "            new_weights = (new_weights.T / xp.sum(new_weights, axis=-1)).T\n",
    "            weights += new_weights\n",
    "            if added_keys is not None:\n",
    "                for key in added_keys:\n",
    "                    self.parameters.pop(key)\n",
    "        weights = (weights.T / xp.sum(weights, axis=-1)).T\n",
    "        new_idxs = xp.empty_like(weights, dtype=int)\n",
    "        for ii in range(self.n_posteriors):\n",
    "            new_idxs[ii] = xp.asarray(\n",
    "                np.random.choice(\n",
    "                    range(self.samples_per_posterior),\n",
    "                    size=self.samples_per_posterior,\n",
    "                    replace=True,\n",
    "                    p=to_numpy(weights[ii]),\n",
    "                )\n",
    "            )\n",
    "        new_samples = {\n",
    "            key: xp.vstack(\n",
    "                [self.data[key][ii, new_idxs[ii]] for ii in range(self.n_posteriors)]\n",
    "            )\n",
    "            for key in self.data\n",
    "        }\n",
    "        event_weights = list(event_weights)\n",
    "        weight_string = \" \".join([f\"{float(weight):.1f}\" for weight in event_weights])\n",
    "        logger.info(f\"Resampling done, sum of weights for events are {weight_string}\")\n",
    "        if return_weights:\n",
    "            return new_samples, weights\n",
    "        else:\n",
    "            return new_samples\n",
    "\n",
    "\n",
    "class RateLikelihood(HyperparameterLikelihood):\n",
    "    \"\"\"\n",
    "    A likelihood for inferring hyperparameter posterior distributions\n",
    "    and estimating rates with including selection effects.\n",
    "    See Eq. (34) of https://arxiv.org/abs/1809.02293 for a definition.\n",
    "    \"\"\"\n",
    "\n",
    "    __doc__ += HyperparameterLikelihood.__init__.__doc__\n",
    "\n",
    "    def _get_selection_factor(self):\n",
    "        ln_l = -self.selection_function(self.parameters) * self.parameters[\"rate\"]\n",
    "        ln_l += self.n_posteriors * xp.log(self.parameters[\"rate\"])\n",
    "        return ln_l\n",
    "\n",
    "    def generate_rate_posterior_sample(self):\n",
    "        \"\"\"\n",
    "        Since the rate is a sampled parameter,\n",
    "        this simply returns the current value of the rate parameter.\n",
    "        \"\"\"\n",
    "        return self.parameters[\"rate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "revised-civilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = bilby.hyper.model.Model([mass.power_law_primary_mass_ratio,\n",
    "                                  iid_spin_magnitude_beta_truncated_gaussian,\n",
    "                                  spin.iid_spin_orientation_gaussian_isotropic,\n",
    "                                  redshift.PowerLawRedshift()])\n",
    "\n",
    "model2 = bilby.hyper.model.Model([mass.power_law_primary_mass_ratio, \n",
    "                                  redshift.PowerLawRedshift()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "oriental-virginia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_conversion1(parameters):\n",
    "    mu = parameters[\"mu_chi\"]\n",
    "    var = parameters[\"sigma_chi\"]\n",
    "    parameters[\"alpha_chi\"] = (mu ** 2 * (1 - mu) - mu * var) / var\n",
    "    parameters[\"beta_chi\"] = (mu * (1 - mu) ** 2 - (1 - mu) * var) / var\n",
    "    return parameters\n",
    "\n",
    "def prior_conversion2(parameters):\n",
    "    mu = parameters[\"mu_chi\"]\n",
    "    var = parameters[\"sigma_chi\"]\n",
    "    added_keys = pd.DataFrame()\n",
    "    parameters[\"alpha_chi\"] = (mu ** 2 * (1 - mu) - mu * var) / var\n",
    "    parameters[\"beta_chi\"] = (mu * (1 - mu) ** 2 - (1 - mu) * var) / var\n",
    "    added_keys[\"alpha_chi\"] = parameters[\"alpha_chi\"]\n",
    "    added_keys[\"beta_chi\"] = parameters[\"beta_chi\"]\n",
    "    return parameters, added_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "rural-offering",
   "metadata": {},
   "outputs": [],
   "source": [
    "priors = PriorDict(conversion_function = prior_conversion1)\n",
    "\n",
    "# mass\n",
    "priors[\"alpha\"] = Uniform(minimum = -4, maximum = 12, latex_label =r'$\\alpha$')\n",
    "priors[\"beta\"] = Uniform(minimum = -4, maximum = 12, latex_label =r'$\\beta_\\mathrm{q}$')\n",
    "priors[\"mmin\"] = Uniform(minimum = 2, maximum = 10, latex_label =r'$m_\\mathrm{min}$ $[M_\\odot]$')\n",
    "priors[\"mmax\"] = Uniform(minimum = 30, maximum = 100, latex_label =r'$m_\\mathrm{max}$ $[M_\\odot]$')\n",
    "\n",
    "# spin magnitude \n",
    "priors[\"mu_chi\"] = Uniform(minimum = 0, maximum = 1, latex_label = r'$\\mu_\\chi$')\n",
    "priors[\"sigma_chi\"] = Uniform(minimum = 0, maximum = 0.25, latex_label = r'$\\sigma^2_\\chi$')\n",
    "priors[\"lambda_chi_peak\"] = 0\n",
    "priors[\"sigma_chi_peak\"] = 0\n",
    "priors[\"amax\"] = 1\n",
    "priors[\"alpha_chi\"] = Constraint(minimum = 1, maximum = 1e5)\n",
    "priors[\"beta_chi\"] = Constraint(minimum = 1, maximum = 1e5)\n",
    "\n",
    "# spin orientation\n",
    "priors[\"xi_spin\"] = Uniform(minimum = 0, maximum = 1, latex_label=r'$\\zeta$')\n",
    "priors[\"sigma_spin\"] = Uniform(minimum = 0, maximum = 4, latex_label=r'$\\sigma_t$')\n",
    "priors[\"zmin\"] = -1\n",
    "\n",
    "# redshift\n",
    "priors[\"lamb\"] = Uniform(minimum=-10, maximum=10, name='lamb', latex_label=\"$\\\\lambda_z$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "automotive-estimate",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:10 bilby INFO    : samples_per_posterior1 23984, samples_per_posterior2 32206\n",
      "18:10 bilby INFO    : log evidences = [-1.97076791e+04 -1.96400883e+04 -1.83249796e+04 -9.11298420e+03\n",
      " -3.64276562e+04 -1.24687154e+04 -1.35702853e+04 -1.39287408e+04\n",
      " -1.39280258e+04 -9.18474989e+03 -5.86245000e+03  8.00000000e+00\n",
      " -1.19764000e+04 -2.80064000e+03 -1.86898000e+03 -1.97996000e+03\n",
      " -2.80438000e+03 -1.17156000e+04 -5.87187000e+03 -7.96574000e+03\n",
      "  1.00000000e+01  1.00000000e+01 -1.19380166e+04 -1.89517000e+03\n",
      " -1.56896400e+04 -2.81296000e+03 -3.91131000e+03 -7.88885000e+03\n",
      " -2.81298000e+03 -2.82154000e+03 -1.59099600e+04 -1.59376800e+04\n",
      " -2.39370500e+04 -2.72098000e+03 -4.79665800e+04 -7.97393000e+03\n",
      " -1.17241000e+04  1.00000000e+01 -2.39979300e+04 -7.99795000e+03\n",
      " -2.83461000e+03 -9.70623700e+04 -1.19876000e+04 -1.59608800e+04]\n",
      "18:10 bilby INFO    : log evidences = [-1.97063920e+04 -1.96390399e+04 -1.83394261e+04 -9.11253073e+03\n",
      " -3.64263007e+04 -1.24702719e+04 -1.35695259e+04 -1.39283146e+04\n",
      " -1.39286865e+04 -9.18430802e+03 -5.85758000e+03 -1.00000000e+00\n",
      " -1.19764000e+04 -2.80026000e+03 -1.86888000e+03 -1.98022000e+03\n",
      " -2.80373000e+03 -1.17141800e+04 -5.87099000e+03 -7.96595000e+03\n",
      " -1.00000000e+00 -1.00000000e+00 -1.19401383e+04 -1.89426000e+03\n",
      " -1.56895800e+04 -2.81281000e+03 -3.91364000e+03 -7.88793000e+03\n",
      " -2.81278000e+03 -2.82454000e+03 -1.59087900e+04 -1.59359400e+04\n",
      " -2.39404800e+04 -2.72098000e+03 -4.79692500e+04 -7.97361000e+03\n",
      " -1.17238000e+04 -1.00000000e+00 -2.39961100e+04 -7.99734000e+03\n",
      " -2.83512000e+03 -9.70605200e+04 -1.19876300e+04 -1.59610200e+04]\n"
     ]
    }
   ],
   "source": [
    "likelihood = HyperparameterLikelihood(posteriors1 = posts,\n",
    "                                      posteriors2 = posts_zero,\n",
    "                                      hyper_prior1 = model1,\n",
    "                                      hyper_prior2 = model2,\n",
    "                                      conversion_function = prior_conversion2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "imperial-antibody",
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood.parameters.update(priors.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "international-fraud",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:10 bilby INFO    : Running for label 'extended_default', output will be saved to 'Results/extended_default'\n",
      "18:10 bilby INFO    : Search parameters:\n",
      "18:10 bilby INFO    :   alpha = Uniform(minimum=-4, maximum=12, name=None, latex_label='$\\\\alpha$', unit=None, boundary=None)\n",
      "18:10 bilby INFO    :   beta = Uniform(minimum=-4, maximum=12, name=None, latex_label='$\\\\beta_\\\\mathrm{q}$', unit=None, boundary=None)\n",
      "18:10 bilby INFO    :   mmin = Uniform(minimum=2, maximum=10, name=None, latex_label='$m_\\\\mathrm{min}$ $[M_\\\\odot]$', unit=None, boundary=None)\n",
      "18:10 bilby INFO    :   mmax = Uniform(minimum=30, maximum=100, name=None, latex_label='$m_\\\\mathrm{max}$ $[M_\\\\odot]$', unit=None, boundary=None)\n",
      "18:10 bilby INFO    :   mu_chi = Uniform(minimum=0, maximum=1, name=None, latex_label='$\\\\mu_\\\\chi$', unit=None, boundary=None)\n",
      "18:10 bilby INFO    :   sigma_chi = Uniform(minimum=0, maximum=0.25, name=None, latex_label='$\\\\sigma^2_\\\\chi$', unit=None, boundary=None)\n",
      "18:10 bilby INFO    :   xi_spin = Uniform(minimum=0, maximum=1, name=None, latex_label='$\\\\zeta$', unit=None, boundary=None)\n",
      "18:10 bilby INFO    :   sigma_spin = Uniform(minimum=0, maximum=4, name=None, latex_label='$\\\\sigma_t$', unit=None, boundary=None)\n",
      "18:10 bilby INFO    :   lamb = Uniform(minimum=-10, maximum=10, name='lamb', latex_label='$\\\\lambda_z$', unit=None, boundary=None)\n",
      "18:10 bilby INFO    :   alpha_chi = Constraint(minimum=1, maximum=100000.0, name=None, latex_label=None, unit=None)\n",
      "18:10 bilby INFO    :   beta_chi = Constraint(minimum=1, maximum=100000.0, name=None, latex_label=None, unit=None)\n",
      "18:10 bilby INFO    :   lambda_chi_peak = 0\n",
      "18:10 bilby INFO    :   sigma_chi_peak = 0\n",
      "18:10 bilby INFO    :   amax = 1\n",
      "18:10 bilby INFO    :   zmin = -1\n",
      "18:11 bilby INFO    : Single likelihood evaluation took 2.977e-02 s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07bb3678edd74076883547f42929b8c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:11 bilby INFO    : Using sampler Dynesty with kwargs {'bound': 'multi', 'sample': 'rwalk', 'verbose': True, 'periodic': None, 'reflective': None, 'check_point_delta_t': 600, 'nlive': 1000, 'first_update': None, 'walks': 100, 'npdim': None, 'rstate': None, 'queue_size': 1, 'pool': None, 'use_pool': None, 'live_points': None, 'logl_args': None, 'logl_kwargs': None, 'ptform_args': None, 'ptform_kwargs': None, 'enlarge': 1.5, 'bootstrap': None, 'vol_dec': 0.5, 'vol_check': 8.0, 'facc': 0.2, 'slices': 5, 'update_interval': 600, 'print_func': <bound method Dynesty._print_func of <bilby.core.sampler.dynesty.Dynesty object at 0x7fe1a0135b20>>, 'dlogz': 0.1, 'maxiter': None, 'maxcall': None, 'logl_max': inf, 'add_live': True, 'print_progress': True, 'save_bounds': False, 'n_effective': None, 'maxmcmc': 5000, 'nact': 5}\n",
      "18:11 bilby INFO    : Checkpoint every check_point_delta_t = 600s\n",
      "18:11 bilby INFO    : Using dynesty version 1.0.1\n",
      "18:11 bilby INFO    : Using the bilby-implemented rwalk sample method with ACT estimated walks\n",
      "18:11 bilby INFO    : Generating initial points from the prior\n",
      "18:12 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "18:22 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "18:33 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "18:43 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "18:53 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "19:03 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "19:13 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "19:24 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "19:34 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "19:44 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "19:54 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "20:05 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "20:15 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "20:25 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "20:35 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "20:45 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "20:56 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "21:06 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "21:16 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "21:27 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "21:37 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "21:47 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "21:58 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "22:08 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "22:18 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "22:29 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "22:39 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "22:49 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "22:59 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "23:10 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "23:20 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "23:31 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "23:41 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "23:52 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "00:02 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "00:12 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "00:22 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "00:33 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "00:43 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "00:54 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "01:04 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "01:14 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "01:25 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "01:35 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "01:45 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "01:56 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "02:06 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "02:17 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "02:27 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "02:38 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "02:48 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "02:59 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "03:09 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "03:09 bilby INFO    : Writing 110 current samples to Results/extended_default/extended_default_samples.dat\n",
      "03:20 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "03:20 bilby INFO    : Writing 127 current samples to Results/extended_default/extended_default_samples.dat\n",
      "03:30 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "03:30 bilby INFO    : Writing 104 current samples to Results/extended_default/extended_default_samples.dat\n",
      "03:41 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "03:41 bilby INFO    : Writing 119 current samples to Results/extended_default/extended_default_samples.dat\n",
      "03:51 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "03:51 bilby INFO    : Writing 185 current samples to Results/extended_default/extended_default_samples.dat\n",
      "04:02 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "04:02 bilby INFO    : Writing 206 current samples to Results/extended_default/extended_default_samples.dat\n",
      "04:12 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "04:12 bilby INFO    : Writing 289 current samples to Results/extended_default/extended_default_samples.dat\n",
      "04:23 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "04:23 bilby INFO    : Writing 365 current samples to Results/extended_default/extended_default_samples.dat\n",
      "04:33 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "04:33 bilby INFO    : Writing 353 current samples to Results/extended_default/extended_default_samples.dat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04:44 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "04:44 bilby INFO    : Writing 443 current samples to Results/extended_default/extended_default_samples.dat\n",
      "04:54 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "04:54 bilby INFO    : Writing 515 current samples to Results/extended_default/extended_default_samples.dat\n",
      "05:05 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "05:05 bilby INFO    : Writing 692 current samples to Results/extended_default/extended_default_samples.dat\n",
      "05:15 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "05:15 bilby INFO    : Writing 463 current samples to Results/extended_default/extended_default_samples.dat\n",
      "05:26 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "05:26 bilby INFO    : Writing 597 current samples to Results/extended_default/extended_default_samples.dat\n",
      "05:37 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "05:37 bilby INFO    : Writing 768 current samples to Results/extended_default/extended_default_samples.dat\n",
      "05:47 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "05:47 bilby INFO    : Writing 1015 current samples to Results/extended_default/extended_default_samples.dat\n",
      "05:58 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "05:58 bilby INFO    : Writing 1276 current samples to Results/extended_default/extended_default_samples.dat\n",
      "06:09 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "06:09 bilby INFO    : Writing 1560 current samples to Results/extended_default/extended_default_samples.dat\n",
      "06:19 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "06:19 bilby INFO    : Writing 1809 current samples to Results/extended_default/extended_default_samples.dat\n",
      "06:30 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "06:30 bilby INFO    : Writing 2382 current samples to Results/extended_default/extended_default_samples.dat\n",
      "06:41 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "06:41 bilby INFO    : Writing 2987 current samples to Results/extended_default/extended_default_samples.dat\n",
      "06:51 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "06:51 bilby INFO    : Writing 3918 current samples to Results/extended_default/extended_default_samples.dat\n",
      "07:02 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "07:02 bilby INFO    : Writing 4946 current samples to Results/extended_default/extended_default_samples.dat\n",
      "07:12 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "07:12 bilby INFO    : Writing 5532 current samples to Results/extended_default/extended_default_samples.dat\n",
      "07:23 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "07:23 bilby INFO    : Writing 5575 current samples to Results/extended_default/extended_default_samples.dat\n",
      "07:34 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "07:34 bilby INFO    : Writing 5546 current samples to Results/extended_default/extended_default_samples.dat\n",
      "07:45 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "07:45 bilby INFO    : Writing 5588 current samples to Results/extended_default/extended_default_samples.dat\n",
      "07:48 bilby INFO    : Written checkpoint file Results/extended_default/extended_default_resume.pickle\n",
      "07:48 bilby INFO    : Writing 5554 current samples to Results/extended_default/extended_default_samples.dat\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:48 bilby INFO    : Sampling time: 13:34:52.615333\n",
      "07:48 bilby INFO    : Summary of results:\n",
      "nsamples: 19454\n",
      "ln_noise_evidence:    nan\n",
      "ln_evidence:    nan +/-  0.151\n",
      "ln_bayes_factor: -105.113 +/-  0.151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = bilby.run_sampler(likelihood = likelihood, priors = priors, sampler='dynesty', \n",
    "                           nlive = 1000, label='extended_default',\n",
    "                           outdir = 'Results/extended_default', clean = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-reproduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlayed_plot_extended(\"Results/O3a_Shanika_extended_results/O3a_Shanika_extended_results.json\", \n",
    "                        \"Results/extended_default/extended_default_result.json\", \n",
    "                        \"extended_default_comparison_with_extended\", \n",
    "                        \"Shanika-extended\", \"Extended_default\", save = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-medline",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "overlayed_plot_extended(\"../Default/Results/O3a_LIGO_default_results/O3a_LIGO_default_results.json\", \n",
    "                        \"Results/extended_default/extended_default_result.json\", \n",
    "                        \"extended_default_comparison_with_O3a_default\", \n",
    "                        \"LVK Default\", \"Extended_default\", save = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
